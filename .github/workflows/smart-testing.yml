name: Smart Testing with Claude

on:
  pull_request:
    types: [opened, synchronize, reopened]
  issue_comment:
    types: [created]
  push:
    branches: [main, develop]

permissions:
  contents: read
  pull-requests: write
  issues: write
  checks: write

jobs:
  # STEP 1: Determine which tests to run
  test-selection:
    name: Select Tests to Run
    runs-on: ubuntu-latest
    outputs:
      test-matrix: ${{ steps.select.outputs.matrix }}
      should-run-unit: ${{ steps.select.outputs.unit }}
      should-run-integration: ${{ steps.select.outputs.integration }}
      should-run-e2e: ${{ steps.select.outputs.e2e }}
    
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Get changed files
        id: changed
        run: |
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            CHANGED=$(git diff --name-only origin/${{ github.base_ref }}..HEAD)
          else
            CHANGED=$(git diff --name-only HEAD~1..HEAD)
          fi
          echo "files<<EOF" >> $GITHUB_OUTPUT
          echo "$CHANGED" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT
      
      - name: Select tests with Claude
        id: select
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          # Use Claude to determine which tests to run
          echo "${{ steps.changed.outputs.files }}" | \
            claude test-runner "Based on these changed files, output JSON with:
            - unit: boolean (should run unit tests)
            - integration: boolean (should run integration tests)
            - e2e: boolean (should run E2E tests)
            - affected_tests: array of specific test files to run" \
            --output-format json > test-selection.json
          
          # Parse output
          echo "unit=$(jq -r '.unit' test-selection.json)" >> $GITHUB_OUTPUT
          echo "integration=$(jq -r '.integration' test-selection.json)" >> $GITHUB_OUTPUT
          echo "e2e=$(jq -r '.e2e' test-selection.json)" >> $GITHUB_OUTPUT
          echo "matrix=$(jq -c '.affected_tests' test-selection.json)" >> $GITHUB_OUTPUT

  # STEP 2: Run tests in pyramid order
  pyramid-tests:
    name: Pyramid Test Execution
    runs-on: ubuntu-latest
    needs: test-selection
    strategy:
      fail-fast: true  # Stop on first failure
      matrix:
        level: [unit, integration, e2e]
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Check if should run
        id: should-run
        run: |
          if [ "${{ matrix.level }}" = "unit" ] && [ "${{ needs.test-selection.outputs.should-run-unit }}" = "false" ]; then
            echo "skip=true" >> $GITHUB_OUTPUT
          elif [ "${{ matrix.level }}" = "integration" ] && [ "${{ needs.test-selection.outputs.should-run-integration }}" = "false" ]; then
            echo "skip=true" >> $GITHUB_OUTPUT
          elif [ "${{ matrix.level }}" = "e2e" ] && [ "${{ needs.test-selection.outputs.should-run-e2e }}" = "false" ]; then
            echo "skip=true" >> $GITHUB_OUTPUT
          else
            echo "skip=false" >> $GITHUB_OUTPUT
          fi
      
      - name: Setup environment
        if: steps.should-run.outputs.skip == 'false'
        uses: actions/setup-node@v4
        with:
          node-version: '20'
      
      - name: Setup Python
        if: steps.should-run.outputs.skip == 'false'
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        if: steps.should-run.outputs.skip == 'false'
        run: |
          # Install Python dependencies if requirements.txt exists
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          else
            echo "No requirements.txt found, skipping Python dependencies"
          fi
          
          # Install Node dependencies
          if [ -f package-lock.json ]; then
            npm ci
          elif [ -f package.json ]; then
            npm install
          else
            echo "No package.json found, skipping Node dependencies"
          fi
      
      - name: Run ${{ matrix.level }} tests
        if: steps.should-run.outputs.skip == 'false'
        run: |
          case "${{ matrix.level }}" in
            unit)
              if [ -d tests/unit ]; then
                pytest tests/unit --json-report --json-report-file=results.json || echo "Python unit tests not found"
              fi
              if [ -f package.json ]; then
                npm run test:unit -- --json --outputFile=results-js.json || echo "JavaScript unit tests not configured"
              fi
              ;;
            integration)
              if [ -d tests/integration ]; then
                pytest tests/integration --json-report --json-report-file=results.json || echo "Python integration tests not found"
              fi
              if [ -f package.json ]; then
                npm run test:integration -- --json --outputFile=results-js.json || echo "JavaScript integration tests not configured"
              fi
              ;;
            e2e)
              if [ -f package.json ]; then
                npx playwright test --reporter=json > results.json || echo "E2E tests not configured"
              else
                echo "E2E tests require package.json"
              fi
              ;;
          esac
          # Always exit successfully for template
          exit 0
      
      - name: Upload test results
        if: always() && steps.should-run.outputs.skip == 'false'
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ matrix.level }}
          path: results*.json

  # STEP 3: Flake detection for failed tests
  flake-detection:
    name: Detect Flaky Tests
    runs-on: ubuntu-latest
    needs: pyramid-tests
    if: failure()
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Download test results
        uses: actions/download-artifact@v4
        with:
          path: test-results
      
      - name: Detect flakes with retry
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          # Analyze failed tests for flake patterns
          for result_file in test-results/**/results*.json; do
            if [ -f "$result_file" ]; then
              # Check if failure is likely a flake
              cat "$result_file" | claude test-runner \
                "Analyze these test failures. For each:
                1. Is it likely a flake? (timeout, network, race condition)
                2. Should we retry it?
                3. Suggested fix
                Output as JSON" \
                --output-format json > flake-analysis.json
              
              # Retry flaky tests
              jq -r '.tests[] | select(.is_flaky == true) | .name' flake-analysis.json | \
              while read test_name; do
                echo "Retrying flaky test: $test_name"
                pytest "$test_name" --reruns 3 --reruns-delay 1
              done
            fi
          done

  # STEP 4: Root cause analysis and fix generation
  failure-analysis:
    name: Analyze Failures & Generate Fixes
    runs-on: ubuntu-latest
    needs: [pyramid-tests, flake-detection]
    if: always()
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Download all test results
        uses: actions/download-artifact@v4
        with:
          path: test-results
      
      - name: Analyze with Claude
        id: analysis
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          # Combine all test results
          cat test-results/**/results*.json > all-results.json 2>/dev/null || echo "{}" > all-results.json
          
          # Generate comprehensive analysis
          cat all-results.json | claude test-runner \
            "Analyze all test failures and generate:
            1. Root cause for each failure
            2. Category (Logic/Dependency/Timeout/Null/Network)
            3. Specific fix for each failure
            4. TODO checklist in markdown
            5. Priority (High/Medium/Low)
            Output as JSON with 'summary' and 'todo_checklist' fields" \
            --output-format json > analysis.json
          
          # Extract TODO checklist for PR comment
          jq -r '.todo_checklist' analysis.json > todo.md
          
          # Set outputs
          echo "has_failures=$(jq -r '.summary.total_failures > 0' analysis.json)" >> $GITHUB_OUTPUT
          echo "todo_list<<EOF" >> $GITHUB_OUTPUT
          cat todo.md >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT
      
      - name: Create PR comment with fixes
        if: github.event_name == 'pull_request' && steps.analysis.outputs.has_failures == 'true'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const analysis = JSON.parse(fs.readFileSync('analysis.json', 'utf8'));
            
            let comment = `## ðŸ” Test Failure Analysis\n\n`;
            comment += `### Summary\n`;
            comment += `- Total Failures: ${analysis.summary.total_failures}\n`;
            comment += `- Flaky Tests: ${analysis.summary.flaky_count}\n`;
            comment += `- Categories: ${Object.entries(analysis.summary.categories).map(([k,v]) => `${k}: ${v}`).join(', ')}\n\n`;
            
            comment += `### Root Cause Analysis\n\n`;
            for (const failure of analysis.failures.slice(0, 5)) {
              comment += `#### âŒ \`${failure.test_name}\`\n`;
              comment += `- **Category**: ${failure.category}\n`;
              comment += `- **Cause**: ${failure.root_cause}\n`;
              comment += `- **Fix**: ${failure.suggested_fix}\n\n`;
            }
            
            comment += `### TODO Checklist\n\n`;
            comment += `${{ steps.analysis.outputs.todo_list }}\n\n`;
            
            comment += `---\n`;
            comment += `*Generated by Claude Test Runner* â€¢ `;
            comment += `[View Full Report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})`;
            
            await github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
      
      - name: Update check status
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const analysis = JSON.parse(fs.readFileSync('analysis.json', 'utf8'));
            
            await github.rest.checks.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              name: 'Test Analysis',
              head_sha: context.sha,
              status: 'completed',
              conclusion: analysis.summary.total_failures > 0 ? 'failure' : 'success',
              output: {
                title: `Test Results: ${analysis.summary.total_failures} failures`,
                summary: analysis.summary.description || 'All tests passed!',
                text: JSON.stringify(analysis, null, 2)
              }
            });

  # STEP 5: Auto-fix simple failures (optional)
  auto-fix:
    name: Auto-Fix Simple Failures
    runs-on: ubuntu-latest
    needs: failure-analysis
    if: |
      github.event_name == 'issue_comment' && 
      contains(github.event.comment.body, '@claude fix tests')
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Download analysis
        uses: actions/download-artifact@v4
        with:
          name: test-analysis
          path: .
      
      - name: Apply automatic fixes
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          # Read analysis
          cat analysis.json | jq -r '.failures[] | select(.auto_fixable == true)' | \
          while read -r failure; do
            test_file=$(echo "$failure" | jq -r '.file')
            fix=$(echo "$failure" | jq -r '.fix_code')
            
            # Apply fix using Claude
            claude test-runner "Apply this fix to $test_file: $fix"
          done
      
      - name: Create fix PR
        run: |
          git config --global user.name "Claude Test Fixer"
          git config --global user.email "claude@example.com"
          git checkout -b fix-tests-${{ github.run_id }}
          git add -A
          git commit -m "fix: Auto-fix test failures

          Applied automatic fixes for:
          $(jq -r '.failures[] | select(.auto_fixable == true) | "- " + .test_name' analysis.json)
          
          Co-authored-by: Claude <noreply@anthropic.com>"
          git push origin fix-tests-${{ github.run_id }}
          
          gh pr create \
            --title "fix: Auto-fix test failures" \
            --body "Automatic fixes applied by Claude Test Runner" \
            --base ${{ github.base_ref || 'main' }}

  # STEP 6: Coverage analysis
  coverage-check:
    name: Coverage Analysis
    runs-on: ubuntu-latest
    needs: pyramid-tests
    if: success()
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup environment
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Generate coverage report
        run: |
          pip install coverage pytest-cov || true
          if [ -d tests ]; then
            coverage run -m pytest || echo "No tests found"
            coverage xml || true
            coverage report --format=markdown > coverage.md || echo "No coverage to report" > coverage.md
          else
            echo "No tests directory found" > coverage.md
          fi
      
      - name: Analyze coverage gaps
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          coverage report --show-missing | \
            claude test-runner "Identify critical uncovered code and generate test cases" \
            > missing-tests.md
      
      - name: Comment coverage on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const coverage = fs.readFileSync('coverage.md', 'utf8');
            const missing = fs.readFileSync('missing-tests.md', 'utf8');
            
            const comment = `## ðŸ“Š Coverage Report\n\n${coverage}\n\n### Missing Test Cases\n\n${missing}`;
            
            await github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

  # STEP 7: Performance benchmarking
  benchmark-tests:
    name: Test Performance Benchmark
    runs-on: ubuntu-latest
    needs: pyramid-tests
    if: success()
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Run benchmark
        run: |
          if [ -d tests ]; then
            pip install pytest-benchmark || true
            pytest --benchmark-only --benchmark-json=benchmark.json || echo '{}' > benchmark.json
          else
            echo '{}' > benchmark.json
          fi
      
      - name: Analyze slow tests
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          cat benchmark.json | claude test-runner \
            "Identify slowest tests and suggest optimizations" \
            > performance-report.md
      
      - name: Store benchmark results
        uses: benchmark-action/github-action-benchmark@v1
        with:
          tool: 'pytest'
          output-file-path: benchmark.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true